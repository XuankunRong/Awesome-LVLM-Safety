# ðŸ¤—ðŸ¤—ðŸ¤— Awesome LVLM Safety [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![License](https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/) [![GitHub stars](https://img.shields.io/github/stars/XuankunRong/Awesome-LVLM-Safety?style=social)](https://github.com/XuankunRong/Awesome-LVLM-Safety)

> Curated list of Large Vision-Language Model Safety resources, aligned with our survey:
> **A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations**

## ðŸ“œ Table of Contents

<details>
<summary>Click to expand</summary>

- [Attacks](#attacks)
- [Defenses](#defenses)
- [Evaluations](#evaluations)

</details>

<h2> <img src="assets/attack.png" alt="Icon" width="20" style="vertical-align:middle"/> Attacks </h2>

* **[2023.06.22] [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213)** [![GitHub stars](https://img.shields.io/github/stars/unispac/visual-adversarial-examples-jailbreak-large-language-models?style=social)](https://github.com/unispac/visual-adversarial-examples-jailbreak-large-language-models)
  * Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal
  * Princeton University, Stanford University
  * [AAAI'24 Oral]
* **[2023.07.19]** **[Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)** [![GitHub stars](https://img.shields.io/github/stars/ebagdasa/multimodal_injection?style=social)](https://github.com/ebagdasa/multimodal_injection)
  * Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov
  * Cornell Tech
  * [arXiv'23]
* **[2023.08.23]** **[On the Adversarial Robustness of Multi-Modal Foundation Models](https://arxiv.org/abs/2308.10741)**
  * Christian Schlarmann, Matthias Hein
  * University of TÃ¼bingen
  * [ICCV'23 AROW]

* **[2023.09.01]** **[Image Hijacks: Adversarial Images can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)** [[Code](https://github.com/euanong/image-hijacks)]
  * Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons
  * Harvard University, Cambridge University , University of California, Berkeley
  * [ICML'24]

* **[2024.01.20]** **[Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images](https://arxiv.org/abs/2401.11170)**
  * Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu
  * Tsinghua University, Tencent Technology (Beijing) Co.Ltd, University of Oxford, Tencent Data Platform,  Peng Cheng Laboratory
  * [ICLR'24]

* **[2024.02.13]** Test-Time Backdoor Attacks on Multimodal Large Language Models **(arXivâ€™24)** [[Paper](https://arxiv.org/abs/2402.08577)] lu2024test
* **[2024.02.22]** Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image **(COLMâ€™24)** [[Paper](https://arxiv.org/abs/2402.14899)] wang2024stop
* **[2024.03.14]** Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models **(ECCVâ€™24)** [[Paper](https://arxiv.org/abs/2403.09792)] li2024images
* **[2024.03.14]** An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models **(ICLRâ€™24)** [[Paper](https://arxiv.org/abs/2403.09766)] luo2024image
* **[2024.05.16]** Adversarial Robustness for Visual Grounding of Multimodal Large Language Models **(ICLRâ€™24 Workshop)** [[Paper](https://arxiv.org/abs/2405.09981)] gao2024adversarial
* **[2024.05.28]** White-box Multimodal Jailbreaks Against Large Vision-Language Models **(MMâ€™24)** [[Paper](https://arxiv.org/abs/2405.17894)]  wang2024white
* **[2024.06.06]** Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt **(arXivâ€™24)** [[Paper](https://arxiv.org/abs/2406.04031)] ying2024jailbreak
* **[2024.06.19]** Enhancing Cross-Prompt Transferability in Vision-Language Models through Contextual Injection of Target Tokens **(arXivâ€™24)** [[Paper](https://arxiv.org/abs/2406.13294)] yang2024enhancing
* **[2024.11.01]** Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models **(arXivâ€™24)**

<h2> <img src="assets/defense.png" alt="Icon" width="18" style="vertical-align:middle"/> Defenses </h2>

<h2> <img src="assets/evaluation.png" alt="Icon" width="27" style="vertical-align:middle"/> Evaluations </h2>
